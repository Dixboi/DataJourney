{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearningNotebookNotes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3RdHErWax6Q"
      },
      "source": [
        "# 30 Days of ML\n",
        "\n",
        "---\n",
        "\n",
        "### Day 8 - [Basic Data Exploration](https://www.kaggle.com/dansbecker/basic-data-exploration?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-8)\n",
        "\n",
        "#### Reading a csv file using pandas\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "csv_file = \"file.csv\"\n",
        "data = pd.read_csv(csv_file)\n",
        "```\n",
        "\n",
        "**Optional:**\n",
        "1.   ```data.describe()``` -> to look at the details of the file\n",
        "2.   ```data.columns()``` -> to look at the columns of the file\n",
        "\n",
        "---\n",
        "\n",
        "### Day 9 - [Your First Machine Learning Model](https://www.kaggle.com/dansbecker/your-first-machine-learning-model?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-9)\n",
        "\n",
        "#### Getting the Prediction Target and Features\n",
        "**NOTE:** for convention the prediction target is ```y``` and the features are ```X```.\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "csv_file = \"file.csv\"\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "y = data.target\n",
        "features = [column1, column2, column3]\n",
        "X = data.features\n",
        "```\n",
        "\n",
        "#### Building a Decision Tree Regressor Model using scikit-learn\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "csv_file = \"file.csv\"\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "y = data.target\n",
        "features = [column1, column2, column3]\n",
        "X = data.features\n",
        "\n",
        "model  = DecisionTreeRegressor(random_state = 1)\n",
        "model.fit(X, y)\n",
        "```\n",
        "**NOTE:** Fitting is finding patterns from the data <br>\n",
        "**Optional:** For checking the prediction\n",
        "```\n",
        "print(X.head())\n",
        "print(model.predict(X.head())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Day 9 - [Model Validation](https://www.kaggle.com/dansbecker/model-validation?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-9)\n",
        "#### Calculating mean absolute error using scikit-learn\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "csv_file = \"file.csv\"\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "y = data.target\n",
        "features = [column1, column2, column3]\n",
        "X = data.features\n",
        "\n",
        "model  = DecisionTreeRegressor(random_state = 1)\n",
        "model.fit(X, y)\n",
        "\n",
        "predicted_targets = model.predict(X)\n",
        "print(mean_absolute_error(y, predicted_targets))\n",
        "```\n",
        "#### Better version of calculating mean absolute error using scikit-learn (train_test_split)\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "csv_file = \"file.csv\"\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "y = data.target\n",
        "features = [column1, column2, column3]\n",
        "X = data.features\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
        "model = DecisionTreeRegressor(random_state = 1)\n",
        "model.fit(train_X, train_y)\n",
        "\n",
        "val_predictions = model.predict(val_X)\n",
        "print(mean_absolute_error(val_y, val_predictions))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Day 10 - [Underfitting and Overfitting](https://www.kaggle.com/dansbecker/underfitting-and-overfitting?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-10)\n",
        "**NOTE:** <br>\n",
        "1. Overfitting is where a model matches the training data almost perfectly, but does poorly in validation and new data.\n",
        "2. Underfitting is where a model fails to capture important patterns in the data.\n",
        "\n",
        "#### To find the best nodes in Decision Tree Regressor where the mean absolute error is the lowest.\n",
        "```\n",
        "def get_mae(leaf_nodes, train_X, val_X, train_y, val_y):\n",
        "  model = DecisionTreeRegressor(max_leaf_nodes = leaf_nodes, random_state = 1)\n",
        "  model.fit(train_X, train_y)\n",
        "  val_predictions = model.predict(val_X)\n",
        "  mae = mean_absolute_error(val_y, val_predictions)\n",
        "\n",
        "  return mae\n",
        "\n",
        "def get_best_mae(set_of_leaf_nodes, train_X, val_X, train_y, val_y):\n",
        "  list_of_maes = []\n",
        "  for leaf_nodes in set_of_leaf_nodes:\n",
        "    list_of_maes.append(get_mae(leaf_nodes, train_X, val_X, train_y, val_y))\n",
        "\n",
        "  return set_of_leaf_nodes[list_of_maes.index[min(list_of_maes)]]\n",
        "```\n",
        "\n",
        "The first function finds the mean absolute error of a given maximum leaf node. The second function finds the maximum leaf node with the lowest mean absolute error.\n",
        "\n",
        "---\n",
        "\n",
        "### Day 10 - [Random Forests](https://www.kaggle.com/dansbecker/random-forests?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-10)\n",
        "#### Building a Random Forest Regressor model instead of Decision Tree Regressor model using scikit-learn\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "csv_file = \"file.csv\"\n",
        "data = pd.read_csv(csv_file)\n",
        " \n",
        "y = data.target\n",
        "features = [column1, column2, column3]\n",
        "X = data.features\n",
        " \n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
        "model = RandomForestRegressor(random_state = 1)\n",
        "model.fit(train_X, train_y)\n",
        " \n",
        "val_predictions = model.predict(val_X)\n",
        "print(mean_absolute_error(val_y, val_predictions))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Day 12 - [Missing Values](https://www.kaggle.com/alexisbcook/missing-values?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-12)\n",
        "\n",
        "**Imputation** - fills in the missing values. For example, filling the missing values with the mean value <br>\n",
        "\n",
        "#### Three approaches in dealing with missing values:\n",
        "1. Approach 1 - Dropping the columns with missing values\n",
        "2. Approach 2 - Imputation\n",
        "3. Approach 3 - An extension to imputation\n",
        "\n",
        "#### Setup code\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv(\"file.csv\")\n",
        "\n",
        "y = data.target\n",
        "\n",
        "melb_predictors = data.drop(['target'], axis=1)\n",
        "X = melb_predictors.select_dtypes(exclude=['object'])\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "```\n",
        "#### Function code to measure quality of each approach\n",
        "```\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
        "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_valid)\n",
        "    return mean_absolute_error(y_valid, preds)\n",
        "```\n",
        "\n",
        "#### Sample code for approach 1\n",
        "```\n",
        "# Get names of columns with missing values\n",
        "cols_with_missing = [col for col in X_train.columns\n",
        "                     if X_train[col].isnull().any()]\n",
        "\n",
        "# Drop columns in training and validation data\n",
        "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
        "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
        "\n",
        "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
        "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
        "```\n",
        "#### Sample code for approach 2\n",
        "```\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Imputation\n",
        "my_imputer = SimpleImputer()\n",
        "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
        "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
        "\n",
        "# Imputation removed column names; put them back\n",
        "imputed_X_train.columns = X_train.columns\n",
        "imputed_X_valid.columns = X_valid.columns\n",
        "\n",
        "print(\"MAE from Approach 2 (Imputation):\")\n",
        "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n",
        "```\n",
        "#### Sample code for approach 3\n",
        "```\n",
        "# Make copy to avoid changing original data (when imputing)\n",
        "X_train_plus = X_train.copy()\n",
        "X_valid_plus = X_valid.copy()\n",
        "\n",
        "# Make new columns indicating what will be imputed\n",
        "for col in cols_with_missing:\n",
        "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
        "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
        "\n",
        "# Imputation\n",
        "my_imputer = SimpleImputer()\n",
        "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
        "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
        "\n",
        "# Imputation removed column names; put them back\n",
        "imputed_X_train_plus.columns = X_train_plus.columns\n",
        "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
        "\n",
        "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
        "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Day 12 - [Categorical Values](https://www.kaggle.com/alexisbcook/categorical-variables?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-12)\n",
        "\n",
        "#### Three approaches in dealing categorical values:\n",
        "1. Approach 1 - Drop categorical values\n",
        "2. Approach 2 - Ordinal Encoding\n",
        "3. Approach 3 - One-hot encoding\n",
        "\n",
        "#### Getting the list of categorical values\n",
        "```\n",
        "s = (X_train.dtypes == 'object')\n",
        "object_cols = list(s[s].index)\n",
        "\n",
        "print(\"Categorical variables:\")\n",
        "print(object_cols)\n",
        "```\n",
        "\n",
        "**NOTE**: The same code from the last lesson is used to measure the quality of each approah (score_dataset)\n",
        "\n",
        "#### Sample code for approach 1\n",
        "```\n",
        "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
        "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
        "\n",
        "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
        "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n",
        "```\n",
        "#### Sample code for approach 2\n",
        "```\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Make copy to avoid changing original data \n",
        "label_X_train = X_train.copy()\n",
        "label_X_valid = X_valid.copy()\n",
        "\n",
        "# Apply ordinal encoder to each column with categorical data\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
        "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
        "\n",
        "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
        "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
        "```\n",
        "#### Sample code for approach 3\n",
        "```\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Apply one-hot encoder to each column with categorical data\n",
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
        "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
        "\n",
        "# One-hot encoding removed index; put it back\n",
        "OH_cols_train.index = X_train.index\n",
        "OH_cols_valid.index = X_valid.index\n",
        "\n",
        "# Remove categorical columns (will replace with one-hot encoding)\n",
        "num_X_train = X_train.drop(object_cols, axis=1)\n",
        "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
        "\n",
        "# Add one-hot encoded columns to numerical features\n",
        "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
        "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
        "\n",
        "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
        "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Day 13 - [Pipelines](https://www.kaggle.com/alexisbcook/pipelines?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-13)\n",
        "\n",
        "Pipelines are a simple way to keep reprocessing and modeling code organized. Benefits of Pipelines:\n",
        "1. Cleaner code\n",
        "2. Fewer bugs\n",
        "3. Easier to productionize\n",
        "4. More options for model validation \n",
        "\n",
        "#### Constructing the pipeline in three steps:\n",
        "\n",
        "#### Step 1: Define preprocessing steps\n",
        "Imputing missing values in numerical data and imputes missing values and applies a one-hot encoding to categorical data.\n",
        "```\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = SimpleImputer(strategy='constant')\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "```\n",
        "#### Step 2: Defining the model\n",
        "```\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "```\n",
        "#### Step 3: Create and evaluate the pipeline\n",
        "```\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('model', model)\n",
        "                             ])\n",
        "\n",
        "# Preprocessing of training data, fit model \n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Preprocessing of validation data, get predictions\n",
        "preds = my_pipeline.predict(X_valid)\n",
        "\n",
        "# Evaluate the model\n",
        "score = mean_absolute_error(y_valid, preds)\n",
        "print('MAE:', score)\n",
        "```\n",
        "### Day 13 - [Cross-Validation](https://www.kaggle.com/alexisbcook/cross-validation?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-13)\n",
        "\n",
        "#### Example of cross-validation\n",
        "```\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Multiply by -1 since sklearn calculates *negative* MAE\n",
        "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
        "                              cv=5,\n",
        "                              scoring='neg_mean_absolute_error')\n",
        "\n",
        "print(\"MAE scores:\\n\", scores)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Day 14 - [XGBoost](https://www.kaggle.com/alexisbcook/xgboost?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-14)\n",
        "\n",
        "#### Sample Code\n",
        "```\n",
        "from xgboost import XGBRegressor as XGBR\n",
        "my_model = XGBR(n_estimators = 100, learning_rate = 0.5, n_jobs = 4)\n",
        "my_model.fit(X_train, y_train\n",
        "            early_stopping_rounds = 5,\n",
        "            eval_set = [(X_valid, y_valid)],\n",
        "            verbose = False)\n",
        "```\n",
        "\n",
        "**n_estimators** --> specifies how many **times** to go through the modeling cycle; too **low** causes **underfitting** while too **high** will cause **overfitting**; typical values are from **100** to **1000**. <br>\n",
        "\n",
        "**early_stopping_rounds** --> offers a way to automatically find the **ideal value** for n_estimators; typical value is **5**. <br>\n",
        "\n",
        "**learning_rate** --> multiplying the predictions of each model from each model by a small number; a **smaller** learning rate and **large** number of estimators will yield more accurate XGBoost models; typical value is **1**. <br>\n",
        "\n",
        "**n_jobs** --> used to build models **faster**; typical value is **number of cores on machine**\n",
        "\n",
        "---\n",
        "\n",
        "### Day 14 - [Data Leakage](https://www.kaggle.com/alexisbcook/data-leakage?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-14)\n",
        "\n",
        "#### Two types of data leakage:\n",
        "1. Target Leakage - occurs when your predictors include data that will not be available at the time you make predictions\n",
        "2. Train-Test Contamination - when training data is getting mixed up with validation data\n",
        "\n",
        "#### Sample code\n",
        "```\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(my_pipeline, X, y, \n",
        "                            cv=5,\n",
        "                            scoring='accuracy')\n",
        "```\n",
        "\n",
        "#### Dropping leaky predictors from dataset\n",
        "```\n",
        "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
        "X2 = X.drop(potential_leaks, axis=1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Day 15 - [Applying the lesson](https://www.kaggle.com/raimondextervinluan/getting-started-with-30-days-of-ml-competition/edit)\n",
        "\n",
        "We will be dealing with a dataset with no missing values but has categorical values(ordinal to be specific).\n",
        "\n",
        "#### Step 1: Import the helpful libraries\n",
        "```\n",
        "# Familiar imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For ordinal encoding categorical variables, splitting data\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For training random forest model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "```\n",
        "\n",
        "#### Step 2: Load the data\n",
        "```\n",
        "# Load the training data\n",
        "train = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\n",
        "test = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\n",
        "\n",
        "# Preview the data\n",
        "train.head()\n",
        "```\n",
        "```\n",
        "# Separate target from features\n",
        "y = train['target']\n",
        "features = train.drop(['target'], axis=1)\n",
        "\n",
        "# Preview features\n",
        "features.head()\n",
        "```\n",
        "#### Step 3: Prepare the data\n",
        "```\n",
        "# List of categorical columns\n",
        "object_cols = [col for col in features.columns if 'cat' in col]\n",
        "\n",
        "# ordinal-encode categorical columns\n",
        "X = features.copy()\n",
        "X_test = test.copy()\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "X[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\n",
        "X_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n",
        "\n",
        "# Preview the ordinal-encoded features\n",
        "X.head()\n",
        "```\n",
        "```\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
        "```\n",
        "#### Step 4: Traing the model\n",
        "```\n",
        "# Define the model \n",
        "model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Train the model (will take about 10 minutes to run)\n",
        "model.fit(X_train, y_train)\n",
        "preds_valid = model.predict(X_valid)\n",
        "print(mean_squared_error(y_valid, preds_valid, squared=False))\n",
        "```\n",
        "#### Step 5: Submitting to the competition\n",
        "```\n",
        "# Use the model to generate predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "output = pd.DataFrame({'Id': X_test.index,\n",
        "                       'target': predictions})\n",
        "output.to_csv('submission.csv', index=False)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0tQqp7Cga35"
      },
      "source": [
        "# Saving output of code to csv\n",
        "```\n",
        "#Use the model to generate predictions\n",
        "predictions = lgbm_model.predict(X_test)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "output = pd.DataFrame({'Id': X_test.index,\n",
        "                       'target': predictions})\n",
        "output.to_csv('submission.csv', index=False)\n",
        "```"
      ]
    }
  ]
}